{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90798f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset,Dataset\n",
    "from torchvision import transforms\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "from stable_baselines3 import PPO\n",
    "from medmnist import ChestMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d47ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED   = 42\n",
    "torch.manual_seed(SEED)  \n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4b7500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SZ = 64\n",
    "IMG_SIZE  = 32\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "611faede",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8ee9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ChestMNIST(split=\"train\", download=True, as_rgb=True, transform=transform)\n",
    "test  = ChestMNIST(split=\"test\", download=True, as_rgb=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571658e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apenas 3 classes classificadas nesse momento\n",
    "#train_idx = [i for i,(x,y) in enumerate(train) if y[0] < NUM_CLASSES]\n",
    "#test_idx  = [i for i,(x,y) in enumerate(test)  if y[0] < NUM_CLASSES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1dd3da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappedDataset(Dataset):\n",
    "    def __init__(self, base_dataset, class_map):\n",
    "        self.data = [(x, class_map[int(y[0])]) for x, y in base_dataset if int(y[0]) in class_map]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05883313",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [int(y[0]) for _, y in train]\n",
    "class_list = sorted(set(all_labels))[:NUM_CLASSES]\n",
    "class_map = {orig: i for i, orig in enumerate(class_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12e787e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MappedDataset(train, class_map)\n",
    "test_dataset = MappedDataset(test, class_map)\n",
    "\n",
    "subset_size = int(0.1 * len(train_dataset))\n",
    "train_subset = Subset(train_dataset, list(range(subset_size)))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SZ, shuffle=True)\n",
    "\n",
    "subset_size = int(0.1 * len(test_dataset))\n",
    "test_subset = Subset(test_dataset, list(range(subset_size)))\n",
    "test_loader = DataLoader(test_subset, batch_size=BATCH_SZ, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(Subset(train, train_idx), batch_size=BATCH_SZ, shuffle=True)\n",
    "#test_loader  = DataLoader(Subset(test,  test_idx),  batch_size=BATCH_SZ, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "279b2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3. Hiper‑parâmetros de RL / PQC ----------\n",
    "N_QUBITS   = 4\n",
    "MAX_DEPTH  = 8          # nº máximo de gates que o agente pode adicionar\n",
    "ACTION_SET = (['rx','ry','rz'] + ['cnot'])   # gates disponíveis\n",
    "N_ACTIONS  = len(ACTION_SET) * N_QUBITS\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caddd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "\n",
    "class CircuitBuilderEnv2(gym.Env):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce0b3e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "\n",
    "class Quantumnet(nn.Module):\n",
    "    def __init__(self, circuit, n_qubits=4, n_classes=3):\n",
    "        super(Quantumnet, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = circuit\n",
    "        self.q_params = nn.Parameter(0.01 * torch.randn(len(circuit)))\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),     # Reduz as dimensões pela metade\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # Reduz as dimensões pela metade\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 32, 32)\n",
    "            dummy_output = self.conv(dummy_input)\n",
    "            flatten_dim = dummy_output.view(1, -1).shape[1]\n",
    "            \n",
    "        self.pre_net = nn.Linear(flatten_dim, n_qubits)\n",
    "        self.post_net = nn.Linear(n_qubits, n_classes)\n",
    "\n",
    "        # Define QNode\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def qnode(inputs, weights):\n",
    "            for i in range(n_qubits):\n",
    "                qml.RY(inputs[i], wires=i)\n",
    "            for i, (gate, wire) in enumerate(circuit):\n",
    "                angle = weights[i]\n",
    "                if gate == 'rx':\n",
    "                    qml.RX(angle, wires=wire)\n",
    "                elif gate == 'ry':\n",
    "                    qml.RY(angle, wires=wire)\n",
    "                elif gate == 'rz':\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "                elif gate == 'cnot':\n",
    "                    qml.CNOT(wires=[wire, (wire + 1) % n_qubits])\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "        self.qnode = qnode\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        x = x.view(x.size(0), 3, 32, 32)\n",
    "        \n",
    "        x = self.conv(x)                          # [batch, 16, 8, 8]\n",
    "        x = x.view(x.size(0), -1)                 # [batch, 16*8*8]\n",
    "        x = self.pre_net(x)                        # [batch, n_qubits]\n",
    "        x = torch.tanh(x) * (torch.pi / 3.0)\n",
    "\n",
    "        q_out = []\n",
    "        for xi in x:\n",
    "            q_result = self.qnode(xi, self.q_params)\n",
    "            q_out.append(torch.stack(q_result).to(DEVICE))   # <- converte aqui\n",
    "\n",
    "        q_out = torch.stack(q_out)\n",
    "        return self.post_net(q_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94a25d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class CircuitBuilderEnv(gym.Env):\n",
    "    def __init__(self, n_qubits=4, max_depth=8):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Define conjunto de ações: gate (rx, ry, rz, cnot) em cada qubit\n",
    "        self.action_set = ['rx', 'ry', 'rz', 'cnot']\n",
    "        self.n_actions = len(self.action_set) * n_qubits\n",
    "\n",
    "        # Espaços do Gym\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0,\n",
    "            high=1.0,\n",
    "            shape=(self.max_depth * 2,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "        \n",
    "    def _add_gate(self, gate, wire):\n",
    "        print(f\"Adicionando gate {gate} no qubit {wire}\")\n",
    "        self.circuit.append((gate, wire))\n",
    "\n",
    "    def _encode_state(self):\n",
    "        \"\"\"Retorna o vetor de estado normalizado com dtype float32\"\"\"\n",
    "        vec = np.zeros((self.max_depth * 2,), dtype=np.float32)\n",
    "        for i, (g, w) in enumerate(self.circuit):\n",
    "            g_id = self.action_set.index(g)\n",
    "            vec[i * 2] = g_id / len(ACTION_SET)\n",
    "            vec[i * 2 + 1] = w / self.n_qubits\n",
    "        return vec\n",
    "\n",
    "    def reset(self):\n",
    "        self.circuit = []\n",
    "        self.step_count = 0\n",
    "        return self._encode_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        gate_id = action // self.n_qubits\n",
    "        wire = action % self.n_qubits\n",
    "        gate = self.action_set[gate_id]\n",
    "\n",
    "        if len(self.circuit) < self.max_depth:\n",
    "            self._add_gate(gate, wire)\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Aqui você pode usar seu modelo de avaliação híbrido\n",
    "        reward = self._evaluate_current_circuit()\n",
    "        done = self.step_count >= self.max_depth\n",
    "        obs = self._encode_state()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def _evaluate_current_circuit(self):\n",
    "        \"\"\"\n",
    "        Define a lógica para calcular recompensa com base no circuito atual.\n",
    "        Aqui está um placeholder (exemplo fixo).\n",
    "        \"\"\"\n",
    "        # Em produção, chame um modelo de avaliação com o circuito atual\n",
    "        return 1.0 if len(self.circuit) == self.max_depth else 0.0\n",
    "\n",
    "    @property\n",
    "    def circuit(self):\n",
    "        return self._circuit\n",
    "\n",
    "    @circuit.setter\n",
    "    def circuit(self, value):\n",
    "        self._circuit = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c151e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitBuilderEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CircuitBuilderEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(N_ACTIONS)\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(MAX_DEPTH * 2,), dtype=np.float32)\n",
    "        self.train_iter = iter(train_loader)\n",
    "        #elf.reset()\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "        \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.circuit = []\n",
    "        self.steps = 0\n",
    "        self.state = np.zeros(MAX_DEPTH * 2, dtype=np.float32)\n",
    "        return self.state,{}\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        gate_idx = action // N_QUBITS\n",
    "        wire = action % N_QUBITS\n",
    "        gate = ACTION_SET[gate_idx]\n",
    "\n",
    "        if self.steps < MAX_DEPTH:\n",
    "            self.circuit.append((gate, wire))\n",
    "            self.steps += 1\n",
    "            self._update_state(gate_idx, wire)\n",
    "\n",
    "        reward = self._evaluate_circuit()\n",
    "        terminated = self.steps == MAX_DEPTH\n",
    "        truncated = False  # Você pode definir lógica de truncamento se quiser\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, terminated, truncated, info\n",
    "\n",
    "    def _update_state(self, gate_idx, wire):\n",
    "        self.state[2 * (self.steps - 1)]     = gate_idx / (len(ACTION_SET) - 1)\n",
    "        self.state[2 * (self.steps - 1) + 1] = wire / (N_QUBITS - 1)\n",
    "\n",
    "    def _evaluate_circuit(self):\n",
    "        if len(self.circuit) == 0:\n",
    "            return 0.0  # ou um pequeno valor de recompensa neutra\n",
    "\n",
    "        try:\n",
    "            x, y = next(self.train_iter)\n",
    "        except StopIteration:\n",
    "            self.train_iter = iter(train_loader)\n",
    "            x, y = next(self.train_iter)\n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.argmax(dim=1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        model = Quantumnet(self.circuit, n_qubits=N_QUBITS).to(DEVICE)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc = (pred == y).float().mean().item()\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad744d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Training RL agent...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal model trained!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;66;03m#-- Train RL agent --\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m#-- Train final model --\u001b[39;00m\n\u001b[32m     59\u001b[39m     train_final_agent()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m model_rl = PPO(\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m, env, \n\u001b[32m      6\u001b[39m                 learning_rate=\u001b[32m3e-4\u001b[39m,n_steps=\u001b[32m2\u001b[39m,\n\u001b[32m      7\u001b[39m                 batch_size=\u001b[32m64\u001b[39m,gamma=\u001b[32m0.95\u001b[39m,\n\u001b[32m      8\u001b[39m                 verbose=\u001b[32m1\u001b[39m,seed=SEED)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining RL agent...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmodel_rl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m model_rl.save(\u001b[33m\"\u001b[39m\u001b[33mrl_agent\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRL agent trained!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Trabalho_individual\\trabalho_individual\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Trabalho_individual\\trabalho_individual\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Trabalho_individual\\trabalho_individual\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Trabalho_individual\\trabalho_individual\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Trabalho_individual\\trabalho_individual\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Trabalho_individual\\trabalho_individual\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Trabalho_individual\\trabalho_individual\\.venv\\Lib\\site-packages\\shimmy\\openai_gym_compatibility.py:116\u001b[39m, in \u001b[36mGymV26CompatibilityV0.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m \u001b[33;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgym_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mCircuitBuilderEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m.steps += \u001b[32m1\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state(gate_idx, wire)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m terminated = \u001b[38;5;28mself\u001b[39m.steps == MAX_DEPTH\n\u001b[32m     35\u001b[39m truncated = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Você pode definir lógica de truncamento se quiser\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mCircuitBuilderEnv._evaluate_circuit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m     x, y = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_iter)\n\u001b[32m     54\u001b[39m x = x.to(DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m y = \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     58\u001b[39m model = Quantumnet(\u001b[38;5;28mself\u001b[39m.circuit, n_qubits=N_QUBITS).to(DEVICE)\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "#-- Train RL agent --\n",
    "def train_agent():\n",
    "    env = CircuitBuilderEnv()\n",
    "\n",
    "    model_rl = PPO(\"MlpPolicy\", env, \n",
    "                    learning_rate=3e-4,n_steps=2,\n",
    "                    batch_size=64,gamma=0.95,\n",
    "                    verbose=1,seed=SEED)\n",
    "    \n",
    "    print(\"Training RL agent...\")\n",
    "    model_rl.learn(total_timesteps=1)\n",
    "    model_rl.save(\"rl_agent\")\n",
    "    print(\"RL agent trained!\")\n",
    "    return model_rl, env\n",
    "\n",
    "#-- Test RL agent --\n",
    "def train_final_agent():\n",
    "    model_rl, best_env = train_agent()  # ← recebe os dois agora\n",
    "    best_circuit = best_env.circuit     # ← acessa o circuito do ambiente\n",
    "\n",
    "    final_model = Quantumnet(best_circuit).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=1e-3)\n",
    "\n",
    "    EPOCHS_SUP = 3\n",
    "    for epoch in range(EPOCHS_SUP):\n",
    "        final_model.train()\n",
    "        for image, labels in train_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            labels = labels.squeeze().long().to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = final_model(image)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #--Sample evaluation--\n",
    "        final_model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for image, labels in test_loader:\n",
    "                image = image.to(DEVICE)\n",
    "                labels = labels.squeeze().long().to(DEVICE)\n",
    "                logits = final_model(image)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS_SUP}, Test Accuracy: {acc:.4f}\")\n",
    "    print(\"Final model trained!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #-- Train RL agent --\n",
    "    train_agent()\n",
    "\n",
    "    #-- Train final model --\n",
    "    train_final_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087956f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 1        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 64       |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.86          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 86            |\n",
      "|    total_timesteps      | 128           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033001974 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | -0.204        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.593         |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00797      |\n",
      "|    value_loss           | 1.36          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.71          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 129           |\n",
      "|    total_timesteps      | 192           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019293651 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | -0.26         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.52          |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00345      |\n",
      "|    value_loss           | 3.24          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.83          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 174           |\n",
      "|    total_timesteps      | 256           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016816612 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | -0.323        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.538         |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00429      |\n",
      "|    value_loss           | 1.19          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.91          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 219           |\n",
      "|    total_timesteps      | 320           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023731776 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | -0.359        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.03          |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00441      |\n",
      "|    value_loss           | 2.19          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.81          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 264           |\n",
      "|    total_timesteps      | 384           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030500628 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | -0.516        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.981         |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.0049       |\n",
      "|    value_loss           | 2.05          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.74          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 315           |\n",
      "|    total_timesteps      | 448           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044517498 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | -0.922        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.466         |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.0066       |\n",
      "|    value_loss           | 1.05          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.72          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 366           |\n",
      "|    total_timesteps      | 512           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043132808 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | -0.397        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.449         |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00388      |\n",
      "|    value_loss           | 0.949         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.73          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 415           |\n",
      "|    total_timesteps      | 576           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018928666 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | 0.07          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.383         |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.00294      |\n",
      "|    value_loss           | 0.835         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.78          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 459           |\n",
      "|    total_timesteps      | 640           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043660868 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.77         |\n",
      "|    explained_variance   | -0.255        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.592         |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.00509      |\n",
      "|    value_loss           | 1.26          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.72          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 503           |\n",
      "|    total_timesteps      | 704           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030043256 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.76         |\n",
      "|    explained_variance   | 0.144         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.595         |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.00217      |\n",
      "|    value_loss           | 1.29          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8             |\n",
      "|    ep_rew_mean          | 2.71          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 546           |\n",
      "|    total_timesteps      | 768           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023001432 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.76         |\n",
      "|    explained_variance   | -0.261        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.436         |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.00267      |\n",
      "|    value_loss           | 0.957         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8            |\n",
      "|    ep_rew_mean          | 2.74         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1            |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 590          |\n",
      "|    total_timesteps      | 832          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006473288 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | -0.0356      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.662        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00566     |\n",
      "|    value_loss           | 1.45         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8           |\n",
      "|    ep_rew_mean          | 2.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 634         |\n",
      "|    total_timesteps      | 896         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001438112 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.533       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8           |\n",
      "|    ep_rew_mean          | 2.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1           |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 681         |\n",
      "|    total_timesteps      | 960         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002586986 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0302      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.15        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.398       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8            |\n",
      "|    ep_rew_mean          | 2.61         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1            |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 733          |\n",
      "|    total_timesteps      | 1024         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009011822 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.566        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000838    |\n",
      "|    value_loss           | 1.16         |\n",
      "------------------------------------------\n",
      "0: ──RY(0.00)─┤  <Z>\n",
      "1: ──RY(0.00)─┤  <Z>\n",
      "2: ──RY(0.00)─┤  <Z>\n",
      "3: ──RY(0.00)─┤  <Z>\n",
      "Epoch 1/3 - Test Accuracy: 0.0000\n",
      "Epoch 2/3 - Test Accuracy: 0.8934\n",
      "Epoch 3/3 - Test Accuracy: 0.8934\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from stable_baselines3 import PPO\n",
    "from gym import spaces\n",
    "\n",
    "# Configs\n",
    "BATCH_SZ = 64\n",
    "IMG_SIZE = 32\n",
    "NUM_CLASSES = 3\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Quantum-aware CNN model\n",
    "class Quantumnet(nn.Module):\n",
    "    def __init__(self, circuit, n_qubits=4, n_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = circuit\n",
    "        self.q_params = nn.Parameter(0.01 * torch.randn(len(circuit)))\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "            dummy_output = self.conv(dummy_input)\n",
    "            flatten_dim = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        self.pre_net = nn.Linear(flatten_dim, n_qubits)\n",
    "        self.post_net = nn.Linear(n_qubits, n_classes)\n",
    "\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def qnode(inputs, weights):\n",
    "            for i in range(n_qubits):\n",
    "                qml.RY(inputs[i], wires=i)\n",
    "            for i, (gate, wire) in enumerate(circuit):\n",
    "                angle = weights[i]\n",
    "                if gate == 'rx':\n",
    "                    qml.RX(angle, wires=wire)\n",
    "                elif gate == 'ry':\n",
    "                    qml.RY(angle, wires=wire)\n",
    "                elif gate == 'rz':\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "                elif gate == 'cnot':\n",
    "                    qml.CNOT(wires=[wire, (wire + 1) % n_qubits])\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 3, IMG_SIZE, IMG_SIZE)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.pre_net(x)\n",
    "        x = torch.tanh(x) * (torch.pi / 3.0)\n",
    "        q_out = [torch.tensor(self.qnode(xi, self.q_params), dtype=torch.float32, device=DEVICE) for xi in x]\n",
    "        q_out = torch.stack(q_out)\n",
    "        return self.post_net(q_out)\n",
    "\n",
    "\n",
    "def draw_circuit(circuit, n_qubits=4):\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def dummy_qnode():\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(0.0, wires=i)\n",
    "        for gate, wire in circuit:\n",
    "            if gate == 'rx':\n",
    "                qml.RX(0.1, wires=wire)\n",
    "            elif gate == 'ry':\n",
    "                qml.RY(0.1, wires=wire)\n",
    "            elif gate == 'rz':\n",
    "                qml.RZ(0.1, wires=wire)\n",
    "            elif gate == 'cnot':\n",
    "                qml.CNOT(wires=[wire, (wire + 1) % n_qubits])\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "    print(qml.draw(dummy_qnode)())\n",
    "\n",
    "# Circuit builder environment\n",
    "class CircuitBuilderEnv(gym.Env):\n",
    "    def __init__(self, n_qubits=4, max_depth=8):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.max_depth = max_depth\n",
    "        self.action_set = ['rx', 'ry', 'rz', 'cnot']\n",
    "        self.n_actions = len(self.action_set) * n_qubits\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(self.max_depth * 2,), dtype=np.float32\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def _add_gate(self, gate, wire):\n",
    "        self._circuit.append((gate, wire))\n",
    "\n",
    "    def _encode_state(self):\n",
    "        vec = np.zeros((self.max_depth * 2,), dtype=np.float32)\n",
    "        for i, (g, w) in enumerate(self._circuit):\n",
    "            g_id = self.action_set.index(g)\n",
    "            vec[i * 2] = g_id / len(self.action_set)\n",
    "            vec[i * 2 + 1] = w / self.n_qubits\n",
    "        return vec\n",
    "\n",
    "    def reset(self):\n",
    "        self._circuit = []\n",
    "        self.step_count = 0\n",
    "        return self._encode_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        gate_id = action // self.n_qubits\n",
    "        wire = action % self.n_qubits\n",
    "        gate = self.action_set[gate_id]\n",
    "\n",
    "        if len(self._circuit) < self.max_depth:\n",
    "            self._add_gate(gate, wire)\n",
    "\n",
    "        self.step_count += 1\n",
    "        reward = self._evaluate_current_circuit()\n",
    "        done = self.step_count >= self.max_depth\n",
    "        return self._encode_state(), reward, done, {}\n",
    "\n",
    "    def _evaluate_current_circuit(self):\n",
    "        return 1.0 if len(self._circuit) == self.max_depth else 0.0\n",
    "    \"\"\"    def _evaluate_current_circuit(self):\n",
    "        # Avaliação real do circuito usando Quantumnet e um mini-treino\n",
    "        model = Quantumnet(self._circuit).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            break  # usa apenas um minibatch\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            acc = (preds == labels).float().mean().item()\n",
    "\n",
    "        return acc  # recompensa com base na acurácia\"\"\"\n",
    "    @property\n",
    "    def circuit(self):\n",
    "        return self._circuit\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "# Training RL agent\n",
    "def train_agent():\n",
    "    env = CircuitBuilderEnv()\n",
    "    model_rl = PPO(\"MlpPolicy\", env, learning_rate=3e-4, n_steps=64,\n",
    "                   batch_size=BATCH_SZ, gamma=0.95, verbose=1, seed=SEED)\n",
    "    model_rl.learn(total_timesteps=1000)\n",
    "    return model_rl, env\n",
    "\n",
    "# Supervised training\n",
    "def train_final_agent():\n",
    "    _, best_env = train_agent()\n",
    "    best_circuit = best_env.circuit\n",
    "    draw_circuit(best_circuit)\n",
    "    final_model = Quantumnet(best_circuit).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        final_model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = final_model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        final_model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(DEVICE),labels.to(DEVICE)\n",
    "                logits = final_model(images)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/3 - Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #-- Train RL agent --\n",
    "    #train_agent()\n",
    "\n",
    "    #-- Train final model --\n",
    "    train_final_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a75a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
